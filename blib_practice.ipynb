{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "def texts_labels_from_folders(path, folders):\n",
    "    \"\"\"\n",
    "    Used when data is in the format of /data/neg/example1.txt and /data/neg/example2.txt\n",
    "    path is the base path\n",
    "    folders is a list of folders, one per label\n",
    "    \n",
    "    i.e. path = 'data'\n",
    "         folders = ['pos', 'neg']\n",
    "         \n",
    "         This will get all files inside /data/pos/ and /data/neg/\n",
    "    \n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(folders):\n",
    "        for fname in glob(os.path.join(path, label, '*.*')):\n",
    "            texts.append(open(fname, 'r').read())\n",
    "            labels.append(label)\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data'\n",
    "folders = ['pos','neg']\n",
    "\n",
    "texts, labels = texts_labels_from_folders(path, folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def texts_labels_from_file(path, header=True):\n",
    "    \"\"\"\n",
    "    Used for when data is in a single csv or tsv file with one example per line and one label (TODO: multi-label)\n",
    "    \n",
    "    path is the location of the csv/tsv file\n",
    "    header is a bool which indicates if the file has a header or not\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if header:\n",
    "        data = pd.read_csv(path)\n",
    "        texts = data[data.columns.values[0]].values\n",
    "        labels = data[data.columns.values[1]].values\n",
    "        return texts, labels\n",
    "    \n",
    "    else:\n",
    "        data = pd.read_csv(path)\n",
    "        texts = data[0].values\n",
    "        labels = data[1].values\n",
    "        return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/train.txt'\n",
    "\n",
    "texts, labels = texts_labels_from_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good is good' 'bad is bad' 'good is great' 'great is great'\n",
      " 'bad is sucks' 'sucks'] [' pos' ' neg' ' pos' ' pos' ' neg' ' neg']\n"
     ]
    }
   ],
   "source": [
    "print(texts, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def spacy_tokenize(texts):\n",
    "    \"\"\"\n",
    "    Turn a list of strings into a list of lists. Each sub-list is the tokenized representation of the string.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = spacy.load('en')\n",
    "    temp = []\n",
    "    for text in texts:\n",
    "        temp.append([tok.text for tok in tokenizer(text)])\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good', 'is', 'good'], ['bad', 'is', 'bad'], ['good', 'is', 'great'], ['great', 'is', 'great'], ['bad', 'is', 'sucks'], ['sucks']]\n"
     ]
    }
   ],
   "source": [
    "print(spacy_tokenize(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, max_words=None, min_count=None, unk_token='<UNK>', pad_token='<PAD>'):\n",
    "        \n",
    "        self.max_words = max_words\n",
    "        self.min_count = min_count\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_count = {}\n",
    "        self.size = 0\n",
    "        \n",
    "        if self.pad_token is not None:\n",
    "            self.add(self.pad_token)\n",
    "        \n",
    "        if self.unk_token is not None:\n",
    "            self.add(self.unk_token)\n",
    "            \n",
    "    def add(self, token):\n",
    "        if token not in self.word2idx:\n",
    "            self.word2idx[token] = self.size\n",
    "            self.idx2word[self.size] = token\n",
    "            self.word_count[token] = 1\n",
    "            self.size += 1\n",
    "        else:\n",
    "            self.word_count[token] += 1\n",
    "            \n",
    "    def build(self, texts):\n",
    "        \"\"\"\n",
    "        Takes a list of tokenized texts, i.e. a list of list of strings, [['one','two','three], ['four,'five','six']]\n",
    "        Builds the vocabulary\n",
    "        \"\"\"\n",
    "        \n",
    "        #build the mapping and counts\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                assert token != self.pad_token, \"Padding token in dataset?\"\n",
    "                assert token != self.unk_token, \"Unknown token in dataset?\"\n",
    "                self.add(token)\n",
    "                \n",
    "        #holds words we want to get rid of\n",
    "        words_to_unk = set()\n",
    "        \n",
    "        #finds words that aren't in the top N words\n",
    "        if self.max_words is not None:\n",
    "            most_common = sorted(self.word_count.items(), key=lambda x: -x[1])\n",
    "            for (word, count) in most_common[self.max_words:]:\n",
    "                words_to_unk.add(word)\n",
    "        \n",
    "        #finds words that appear less than N times\n",
    "        if self.min_count is not None:\n",
    "            most_common = sorted(self.word_count.items(), key=lambda x: x[1])\n",
    "            for (word, count) in most_common:\n",
    "                if count < self.min_count:\n",
    "                    words_to_unk.add(word)\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        if self.unk_token is not None:\n",
    "            words_to_unk.remove(self.unk_token)\n",
    "            \n",
    "        if self.pad_token is not None:\n",
    "            words_to_unk.remove(self.pad_token)\n",
    "            \n",
    "        #only bother doing this if we have unknowns to handle\n",
    "        if len(words_to_unk)>0:\n",
    "            \n",
    "            self.word2idx = {}\n",
    "            self.idx2word = {}\n",
    "            self.word_count = {}\n",
    "            self.size = 0\n",
    "\n",
    "            if self.pad_token is not None:\n",
    "                self.add(self.pad_token)\n",
    "\n",
    "            if self.unk_token is not None:\n",
    "                self.add(self.unk_token)\n",
    "\n",
    "            for text in texts:\n",
    "                for token in text:\n",
    "                    if token not in words_to_unk:\n",
    "                        self.add(token)\n",
    "                    \n",
    "    def numericalize(self, texts):\n",
    "        \n",
    "        self.build(texts)\n",
    "        \n",
    "        temp_texts = []\n",
    "        \n",
    "        for text in texts:\n",
    "            temp_texts.append([self.word2idx.get(token, self.word2idx[self.unk_token]) for token in text])\n",
    "    \n",
    "        return temp_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(min_count=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = spacy_tokenize(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>', '<UNK>', 'sucks'}\n",
      "{'sucks'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 3, 2], [4, 3, 4], [2, 3, 5], [5, 3, 5], [4, 3, 1], [1]]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.numericalize(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 1, '<UNK>': 1, 'bad': 3, 'good': 3, 'great': 3, 'is': 5}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>', 1: '<UNK>', 2: 'good', 3: 'is', 4: 'bad', 5: 'great'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_set == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
